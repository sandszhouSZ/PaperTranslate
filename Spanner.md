# Spanner

# 摘要
Spanner是Google的可扩展的、多版本、全局分布式的、同步-复制的数据库。Spanner是 将数据在全球范围内进行分布并且可以支持 外部-一致性 分布式事务的第一个系统。这篇文章描述了 Spanner是如何设计的、它的特性集合、各种设计决策的基本原理 以及可以解决时钟不确定性问题的新的时间Api。这个Api以及其实现 是 支持 外部一致性 以及一些列特性的关键：非阻塞的读，lock-free 只读事务，原子的模式变化，贯穿Spanner的所有特性。
 
# 1 介绍
Spanner是一个可扩展的，全球分布式的数据库，设计，实现，并且部署均由Google亲自操刀。在抽象的最高层，Spanner是一个将数据分布跨越全世界不同数据中心间大量Paxos状态机Sets集群的数据库。副本被用于全局可用性和地址局部性（说人话就是：副本用于在全球范围内多个数据中心实现数据布局和访问高可用）；client自动的在副本间进行failover。Spanner自动的根据数据的数量 或者 节点的变更 在节点间对数据重新布局 并且 自动的在节点间（甚至在数据中心间）进行数据迁移来均衡负载并且对异常进行副本修复（就是根据数据的份数变化，新节点的加入，负载的变化，异常发生等等进行数据重分布）。Spanner被设计用来实现： 可以扩展到横跨数百个数据中心的数百万台节点以及万亿行的数据库。
 
应用程序通过将他们的数据复制到一个洲多个数据中心甚至横跨多个洲可以使用Spanner来实现高可用甚至是出现很大范围内的自然灾害。我们第一个用户是F1，Google一个重写的广告后端。F1将数据分布在美国国土范围内的存储了5个副本，大多数其他的应用或许可以尝试将数据分布在一个地理区域内具有相对独立故障模式的3到5个数据中心之间。这样，大多数的应用可以在高可用的基础上实现很低的业务延迟，同时在1~2个数据中心异常时仍然可以对外提供服务。

Spanner的核心聚焦在 如何管理副本横跨多数据中心的数据，同时我们也在Google现有的分布式系统基础设施上花费大量时间设计并且实现重要的数据库特性。尽管Bigtable已经非常适合Google现有的很多项目，但我们也经常会收到用户对Bigtable对有些场景难以应用的一些抱怨：`那些应用往往有复杂的模式`，或者`有些在多个数据中心拥有副本的应用需要强一致`.很多应用只能选择具有半-结构数据模型并且同步复制的Megastore，尽管其写吞吐相对而言非常差。最终，Spanner从Bigtable的带版本的key-value存储演变为 `时态多版本数据库`。数据存储在图式化的半-关系型表中；数据带有版本，每个版本是其提交时的时间戳；旧版本的数据通过可配置的垃圾-回收策略可以及时回收；并且应用程序可以从旧时间戳的数据。Spanner支持通用的事务并且提供SQL-based的查询语言。
 
作为一个全局分布的数据库系统，Spanner提供了几个有趣的特性。1） 应用自己可以细粒度的动态调整数据的副本配置。应用可以通过约束来控制哪个数据中心包含那份数据，数据中心应该离用户有多远（控制读延迟），每个副本之间的距离有多远（控制写延迟），要保证至少有多少个副本（控制持久性，可用性以及读性能）。数据也可以被系统动态并且透明的在数据中心之间移动以便均衡数据中心之间的资源使用率。 2 ) Spanner有两个在分布式数据库中难以应用的特性： 其提供外部一致性读和写，并且在某一个时间戳提供跨数据库的全局一致的读能力。 这些特性使得Spanner可以支持一致性的备份，一致性的MapReduce执行，原子的模式更新，所有的操作都是全局范围内，即使是正在进行的事务。
 
这些特性是通过 Spanner分配一个`全局范围内有意义的提交时间戳`给事务来保证的,即使事务可能是分布式的。时间戳反映了串行化的顺序。并且，串行化的顺序满足外部一致性（或者等价的表述，串行化特性）：如果一个事务T1提交在另外一个事务T2开始之前，T1的提交时间戳会小于T2的提交时间戳。Spanner是全局范围内第一个提供这个保证的系统。
 
能实现所有这些特性的关键是一个新的称为：TrueTime的API和其实现。这个API直接解决了时钟不确定问题，Spanner的时间戳的保证依赖其具体实现所能提供的能力。如果这个不确定性非常大，Spanner会减速并等待这种不确定性过去。Google的集群管理软件提供了TrueTime API的一种实现，这种实现通过使用多重现代时钟引用（GPS原子钟）可以保证不确定性足够小（一般情况下会小于10ms）。
 
第 2部分描述了Spanner实现的架构，特性集合，融入其设计之中的工程决策。 
第3部分描述我们新的TrueTime API以及其实现的草图。
第4部分描述Spanner如何使用TrueTime来实现外部-一致性的分布式事务，lock-frer只读事务，原子的模式更新。
第 5部分提供了关于Spanner性能的一些基准测试以及TrueTime的表现，讨论了F1的经历。
第6，7，8部分描述了相关的以及未来的工作，最后总结出我们的结论。
 
# 2 实现
这一部分：1） 描述Spanner实现的系统架构以及理论基础；2）然后描述了目录抽象，其被用于管理副本和局部性，目录抽象是数据移动的单位；3）最后描述了我们的数据模型，为什么Spanner看起来更像关系型数据库，而不是key-value存储，以及应用如何控制数据的本地性。
 
一个Spanner的部署称为一个universe。表明Spanner对数据的管理是全球范围的，系统只有少数运行的universe。我们当前运行了一个test/playground universe，一个development/production universe，以及一个只用于生成环境的universe。

Spanner通过一系列zones的集合组织在一起，每个zone只是类似Bigtable集群部署的一种粗略的模拟。Zones是系统部署的单位。一系列zone也是数据可以被复制到的位置集合。zones可以作为一个新的数据中心添加进universe系统，也可以作为旧的数据中心从一个运行的系统被删除。zones也是物理隔离性的单位：一个数据中心可能包含一个或者多个zones，比如，如果不同的应用的数据必须分区到同义数据中心的不通集群节点之间。
 
表1展示了Spanner universe的节点情况，一个zone包含一个zonemaster以及成百上千的spannerservers。前者负责将数据分配到不同的spanner节点；后者向client提供数据。每个zone的位置代理服务被用于向client提供定位对应数据的spannerserver位置。universe master和placement driver目前是单点。 universe master主要展示所有zones交互的debug的状态信息。placement driver以分钟的粒度处理数据跨zone的自动迁移。placement driver周期性的和spanserver交互来发现需要移动的数据，或者去满足更新的副本约束或者为了均衡。为了空间的原因，我们只详细描述spanserver。
 
# 2.1 Spanserver 软件栈
 
 
