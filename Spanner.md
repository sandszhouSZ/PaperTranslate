# Spanner

# 摘要
Spanner是Google的可扩展的、多版本、全局分布式的、同步-复制的数据库。Spanner是 将数据在全球范围内进行分布并且可以支持 外部-一致性 分布式事务的第一个系统。这篇文章描述了 Spanner是如何设计的、它的特性集合、各种设计决策的基本原理 以及可以解决时钟不确定性问题的新的时间Api。这个Api以及其实现 是 支持 外部一致性 以及一些列特性的关键：非阻塞的读，lock-free 只读事务，原子的模式变化，贯穿Spanner的所有特性。
 
# 1 介绍
Spanner是一个可扩展的，全球分布式的数据库，设计，实现，并且部署均由Google亲自操刀。在抽象的最高层，Spanner是一个将数据分布跨越全世界不同数据中心间大量Paxos状态机Sets集群的数据库。副本被用于全局可用性和地址局部性（说人话就是：副本用于在全球范围内多个数据中心实现数据布局和访问高可用）；client自动的在副本间进行failover。Spanner自动的根据数据的数量 或者 节点的变更 在节点间对数据重新布局 并且 自动的在节点间（甚至在数据中心间）进行数据迁移来均衡负载并且对异常进行副本修复（就是根据数据的份数变化，新节点的加入，负载的变化，异常发生等等进行数据重分布）。Spanner被设计用来实现： 可以扩展到横跨数百个数据中心的数百万台节点以及万亿行的数据库。
 
应用程序通过将他们的数据复制到一个洲多个数据中心甚至横跨多个洲可以使用Spanner来实现高可用甚至是出现很大范围内的自然灾害。我们第一个用户是F1，Google一个重写的广告后端。F1将数据分布在美国国土范围内的存储了5个副本，大多数其他的应用或许可以尝试将数据分布在一个地理区域内具有相对独立故障模式的3到5个数据中心之间。这样，大多数的应用可以在高可用的基础上实现很低的业务延迟，同时在1~2个数据中心异常时仍然可以对外提供服务。

Spanner的核心聚焦在 如何管理副本横跨多数据中心的数据，同时我们也在Google现有的分布式系统基础设施上花费大量时间设计并且实现重要的数据库特性。尽管Bigtable已经非常适合Google现有的很多项目，但我们也经常会收到用户对Bigtable对有些场景难以应用的一些抱怨：`那些应用往往有复杂的模式`，或者`有些在多个数据中心拥有副本的应用需要强一致`.很多应用只能选择具有半-结构数据模型并且同步复制的Megastore，尽管其写吞吐相对而言非常差。最终，Spanner从Bigtable的带版本的key-value存储演变为 `时态多版本数据库`。数据存储在图式化的半-关系型表中；数据带有版本，每个版本是其提交时的时间戳；旧版本的数据通过可配置的垃圾-回收策略可以及时回收；并且应用程序可以从旧时间戳的数据。Spanner支持通用的事务并且提供SQL-based的查询语言。
 
作为一个全局分布的数据库系统，Spanner提供了几个有趣的特性。1） 应用自己可以细粒度的动态调整数据的副本配置。应用可以通过约束来控制哪个数据中心包含那份数据，数据中心应该离用户有多远（控制读延迟），每个副本之间的距离有多远（控制写延迟），要保证至少有多少个副本（控制持久性，可用性以及读性能）。数据也可以被系统动态并且透明的在数据中心之间移动以便均衡数据中心之间的资源使用率。 2 ) Spanner有两个在分布式数据库中难以应用的特性： 其提供外部一致性读和写，并且在某一个时间戳提供跨数据库的全局一致的读能力。 这些特性使得Spanner可以支持一致性的备份，一致性的MapReduce执行，原子的模式更新，所有的操作都是全局范围内，即使是正在进行的事务。
 
这些特性是通过 Spanner分配一个`全局范围内有意义的提交时间戳`给事务来保证的,即使事务可能是分布式的。时间戳反映了串行化的顺序。并且，串行化的顺序满足外部一致性（或者等价的表述，串行化特性）：如果一个事务T1提交在另外一个事务T2开始之前，T1的提交时间戳会小于T2的提交时间戳。Spanner是全局范围内第一个提供这个保证的系统。
 
能实现所有这些特性的关键是一个新的称为：TrueTime的API和其实现。这个API直接解决了时钟不确定问题，Spanner的时间戳的保证依赖其具体实现所能提供的能力。如果这个不确定性非常大，Spanner会减速并等待这种不确定性过去。Google的集群管理软件提供了TrueTime API的一种实现，这种实现通过使用多重现代时钟引用（GPS原子钟）可以保证不确定性足够小（一般情况下会小于10ms）。
 
第 2部分描述了Spanner实现的架构，特性集合，融入其设计之中的工程决策。 
第3部分描述我们新的TrueTime API以及其实现的草图。
第4部分描述Spanner如何使用TrueTime来实现外部-一致性的分布式事务，lock-frer只读事务，原子的模式更新。
第 5部分提供了关于Spanner性能的一些基准测试以及TrueTime的表现，讨论了F1的经历。
第6，7，8部分描述了相关的以及未来的工作，最后总结出我们的结论。
 
# 2 实现
这一部分：1） 描述Spanner实现的系统架构以及理论基础；2）然后描述了目录抽象，其被用于管理副本和局部性，目录抽象是数据移动的单位；3）最后描述了我们的数据模型，为什么Spanner看起来更像关系型数据库，而不是key-value存储，以及应用如何控制数据的本地性。
 
一个Spanner的部署称为一个universe。表明Spanner对数据的管理是全球范围的，系统只有少数运行的universe。我们当前运行了一个test/playground universe，一个development/production universe，以及一个只用于生成环境的universe。

Spanner通过一系列zones的集合组织在一起，每个zone只是类似Bigtable集群部署的一种粗略的模拟。Zones是系统部署的单位。一系列zone也是数据可以被复制到的位置集合。zones可以作为一个新的数据中心添加进universe系统，也可以作为旧的数据中心从一个运行的系统被删除。zones也是物理隔离性的单位：一个数据中心可能包含一个或者多个zones，比如，如果不同的应用的数据必须分区到同义数据中心的不通集群节点之间。
 
表1展示了Spanner universe的节点情况，一个zone包含一个zonemaster以及成百上千的spannerservers。前者负责将数据分配到不同的spanner节点；后者向client提供数据。每个zone的位置代理服务被用于向client提供定位对应数据的spannerserver位置。universe master和placement driver目前是单点。 universe master主要展示所有zones交互的debug的状态信息。placement driver以分钟的粒度处理数据跨zone的自动迁移。placement driver周期性的和spanserver交互来发现需要移动的数据，或者去满足更新的副本约束或者为了均衡。为了空间的原因，我们只详细描述spanserver。
 
# 2.1 Spanserver 软件栈
这部分聚焦在spanserver的实现上来展示副本和分布式事务如何在基于Bigtable实现中进行分层。 软件栈在表2总进行展示。最底下，每个spanserver负责100-1000个tablet的实例的维护。一个tablet和Bigtable中的tablet抽象类似，tablet内部实现了很多下面的映射：（key：string，timestamp：int64） -> string,和Bigtable不同的是，Spanner将时间戳分配给数据，这是相比较key-value存储，Spanner为什么更像一个多-版本数据的重要依据。
 
一个tablet的状态存储在一系列类似B-tree的文件以及一个WAL（提前写日志）中，存储在分布式文件系统的所有数据称为colossus（GFS的后继版本）。
 
 为了支持副本，每个spanserver在每个tablet基础上实现了一个简单的paxos状态机。（一个早期的Spanner前身每个tablet支持多个paxos状态机，可以允许更灵活的副本配置。那种设计过于复杂最终让我们选择放弃）每个状态机在其关联的tablet中存储了他的元数据和日志。我们的Paxos实现通过基于时间的leader选举支持长期有效的leader，时间戳的有效时间是10S。当前的Spanner实现对每个Paxos写记录两次：这种实现主要是基于简单性考虑，我们最总会对此进行优化。我们将Paxos实现为pipelined方式，以便在WAN时延的场景下提高Spanner的吞吐；但是Paxos对write的应用是按照顺序的（我们依赖于第4部分的事实）。
 
 Paxos状态机被用于实现一个一致性的副本映射。对每个备份的key-value 映射状态存储于对应的tablet中。写操作必须在leader节点初始化Paxos协议；读直接访问任意一个足够新的副本的底层的tablet即可。副本的集合组成一个Paxos组。
 
 每个成为leader的副本的节点上，其spanserver实现了一个`lock 表`去实现并发控制。这个lock表包含了两-阶段锁的状态：它将key的范围映射到lock的状态（注意到 拥有一个长期有效的Paxos leader是管理lock表的关键）。在Bigtable和Spanner中，我们设计了长期-有效的事务（比如，为了报表生成，可能需要数十分钟），其在冲突存在时在乐观并发控制逻辑下会执行非常低效。需要同步的操作，比如事务操作的读，获得了lock表的锁；其他的操作绕开这个lock表。
 
 在每个成为leader的副本上，其Spanserver也会实现一个`事务管理着`来支持分布式事务。事务管理者被用于实现一个参与者leader；该组的其他副本会成为参与者slave。如果一个事务只涉及到一个Paxos组（对于绝大多数事务），可以绕过事务管理者，这是因为lock表和Paxos一起提供了事务能力。如果一个事务涉及到多余一个Paxos组，那些组的leaders需要协商实现两阶段提交。一个参与者组被选为参与者：这个组的参与者leader将被成为coordinator leader，该组的其他成员称为：coordinator slave。每个事务管理者的状态存储在底层的Paxos group中（因此也是副本化的）。
 
 # 2.2 目录（bucket）和定位
在一系列key-value映射的最上层,Spanner的实现支持一个 bucketing 抽象称为：目录，其是一系列共享同一前缀的连续的keys的集合。（选择directory是一个历史偶然；一个更好的名称为bucket）。我们将在2.3讲解前缀的来源。支持目录允许应用可以通过仔细的选择key来控制数据的位置布局。
 
 一个目录是数据布局的基本单位。一个目录下的所有数据具有同样的副本配置。当数据在Paxos组之间移动意味这数据在不同的目录下移动。比如表3。Spanner可能从Paxos组将一个目录进行移动以便负载均衡；将频繁访问的目录放在一个组中；或者将一个目录移动到离他的接入点更近的地方。目录可以在client的操作正在进行时进行被移动。你可以设想一个50MB的目录可以在几秒内移动完毕。
 
一个事实是 Paxos组可能包含多个目录，表明一个spanner表和Bigtable表不一样：前者不一定是行空间在一个字典序连续的分区。而且，一个Spanner tablet是一个可以将行空间上的多个分片进行封装的容器。我们做这个决定以便可以将频繁访问的多个目录存储在一起。
 
 MoveDir是一个用于将目录在Paxos组之间移动的后台任务。Movedir也被用于向Paxos group中添加或者从Paxos group中删除副本，因为Spanner尚不支持 Paxos内的配置变化。Movedir也不是一个单独的事务，这是为了在数据批量移动时避免阻塞进行中的读以及写。作为替代，movedir表明一个事实：开始移动数据并且将在后台移动。当已经移动绝大多数数据后，他将使用锁来原子的移动剩余的数据并且更新两个Paxos组的元数据。
 
 
一个目录也是应用可以配置的副本地理特性的最小单位（数据定位）。我们的定位-规格语言的设计与管理副本配置是不相关的。系统管理控制两个纬度：副本的数量 和 副本的类型，副本的地理位置。他们在这两个纬度创建了很多命名的菜单选项（比如南美，replicated 5 ways with 1 witness)）。一个应用控制着数据如何复制，通过每个数据库的 与/或标志不同的目录和那些选项的组合。比如，一个应用可能存储每个终端用户的数据在自己的目录中，最终须臾用户A的数据在欧洲有3个副本，同时用户B的数据在南美有5个副本。
 
为了更加清晰的说明我们的描述已经简化很多细节。事实上，如果目录逐步变大Spanner会将目录分布在多个片段中。不同的片段将被不通的Paxos组锁管理（因此也是不同的节点）。Movedir实际上是不通组之间的片段的移动，而不是整个目录。
 
# 2.3 数据模型
Spanner对应用呈现了下面的一系列数据特性：1）一个基于模式的半-关系表的数据模型，2） 一个查询语言，3）通用目标的事务。支持这些特性的因素是由很多因素决定的。支持模式的半-关系表和同步复制通过Megastore进行支持。Google内部至少有300个应用使用Megastore（即使它相对低的性能），主要是因为其数据模型相比较Bigtable比较简单，而且其支持跨数据中心的同步复制。（Bigtable只支持跨数据中心的最终一致性的复制），其中使用Megastore最注明的应用有Gmail，Picasa，Claendar，Android市场以及AppEngine；2） 而支持类SQL查询语言也是非常明确的一个需求，主要得益于Dremel作为交互式数据分析工作的流行； 3）最后，Bigtable由于不支持跨-行事务导致用户频繁的抱怨，Percolator即作为尝试解决这种问题的一个产品。一些作者抱怨由于引入的`性能或者可用性问题`Percolator通用的两阶段提交太浪费。我们坚信`与其经常 因为缺少事务支持而不得不让开发人员额外编写很多逻辑相比 让应用开发人员处理由于滥用事务而导致的性能问题瓶颈会是更好的选择`。使用基于Paxos两-阶段提交减少了可用性问题。
 
应用数据模型建立在 底层基于目录-桶结构的key-value映射的实现之上。一个应用在universe之上创建一个或者多个数据库。每个数据库可以包含一个 无限数量有模式的表。表看起来向关系型数据系统的表，包括行，列以及带版本的数据。我们并不单算深入到Spanner的查询语言。它看起来像SQL并代友一些支持protocol-buffer-valued字段的扩展。
  
Spanner的数据模型不是纯粹的关系型，意味着行必须有名字。更精确的讲，每个表被要求有一个或者多个主键有序的列。这个需求使得Spanner看起来仍然向一个key-value存储：主键形成了行的名字，每个表定义了一个从主键列到非主键列的映射。一个行只有当其行key的一些值被定义时（甚至为NULL）才有存在的必要。之所以要强迫使用这种结构是因为这样才能让上层应用通过对key的选择来控制数据的布局。
 
表4包含了用于存储每个用户，每个相册的照片元数据的Spanner模式实例。模式语言和Metastore的非常类似，只是有一个额外的需求：每个Spaner数据库必须被client分区到一层或者多层的表中（User表和Album表）。客户端应用通过`interleave in声明`声明数据库模式的层次。层次的最顶层的表称为：目录表。该表中的key值为K的每一行和下层表的所有从K按照字典序开始行组成了一个目录。ON DELETE CASCADE表明 删除该目录表的一行也会删除所有相关的chid行。这个表也展示了式例数据库的交错的布局：比如，Albums（2，1）代表了Albums表user_id = 2,album_id = 2的行。由于这允许client端描述 存在于多个表之间的局部关系(这对分片的，分布式的数据库的性能非常有帮助)，所以表的这种交错组成目录非常有意义。否则，Spanner将无法得到最重要的局部关系特征。
 
# 3 TrueTime
这一部分，我们描述TrueTime API以及大致描述其实现。我们将详细部分放在另外一篇Paper中：我们的目标是展示拥有这个API的威力。表1列举了API的接口。TrueTime代表TTinterval的时间，其代表了不确定性的时间窗口（和标准的时间接口给client明确的时间不一样）。一个TTinterval的端点是TTstamp类型。TT.now()方法返回了一个TTinterval，该周期保证包含`TT.now()将被调用`的绝对时间。时钟周期和UNIX时间是类似的。定义了瞬时错误的边界为<,<为间隔宽度的一半，平均错误宽度为《。TT.after（）以及TT.before()方法是通过TT.now封装后的结果。
 
一个事件e的绝对时间用Tabs（e）。为了正式的描述，TrueTime确保 对于一次调用tt=TT.now()，tt.earliest <= Tabs(Enow) <= tt.latest,Enow是调用的事件。
 
TrueTime使用的时间引用依赖于GPS和原子时钟。TrueTime使用 两个规格的时间引用因为他们拥有不同的错误模型。GPS引用-源可能发生的异常诱因包括：天线和接收异常，局部无线电干扰，相关异常（比如设计异常比如 不正确的跳变处理和欺骗），GPS系统停电。原子钟的失效场景和GPS没有关系，其在下面场景可能失效：由于频率错误导致的长时间时间漂移。
 
 TrueTime通过 每个数据中心一系列time主节点机器以及每个机器上一个timeslave守护进程组成。 大多数master有一个专用天线的GPS接收器；这些master在物理上分布部署以便最小化天线故障，无线电干扰和欺骗的影响。其他的master(也称为末日大师)则安装原子钟。原子钟并不非常昂贵：末日大师的花费和GPS的花费在一个数量级。所有master上的时间引用周期性的相互对比。每个master也会交叉检查另外master的时间引用相对自己本里时钟的偏移并且如果存在巨大的差异时调整自己。在同步时，末日大师通知一个 由保守的应用的最差的时钟漂移确定的 缓慢增长的不确定性周期；GPS master通常通知不确定性接近为0.
 
每个守护进程轮许一系列master来减少来自其他master的错误的概率。一些GPS master来自附近的数据中心；其他的GPS master来自很远的数据中心，并且一些来自末日大师。守护进程应用一系列Marzullo’s算法来探测并且拒绝异常，并且将本地机器时钟同步给异常节点。为了保证不破坏本地时钟，展示频率超过部件配置和操作环境配置的最差界限的异常节点将被剔除。
 
在同步是，一个守护进程展示一个缓慢增长的时间不确定性。<是来自保守应用最坏情况的本地时钟漂移，<也依赖time-master的不确定性和到其他time master的通信的时延。在我们的生产环境中，<一般是时间的锯齿型函数。在每个轮询周期在1-7ms之间变动。《因此在大多数情况下4ms。守护进程的轮许周期一般为30s，当前的应用偏移率设置在200微妙/秒，这些一起将时间界限控制在0-6ms之间。有1ms来自时钟master之间的交互延迟。在异常情况下这种偏移是可能的。比如，偶然的时钟-master不可用可能导致数据中心-范围增加<。类似的，过载的设备和网络链路可能导致偶然的<毛刺。
 
# 4 并发控制
这部分描述TrueTime如何被用来在并发控制中保证正确性，以及这些特性被用以实现如下的特性：外部的一致性，无锁只读事务，非阻塞读。这些特性保证比如： 全局数据库在时间戳t的审计读将严格的看到在t时间点前提交的每个事务。
 
继续描述，从Paxos视角(我们接下来用Paxos写描述)来区分来自Spanner client的写非常重要。比如，两阶段提交在prepare阶段生成了一个Paxos写,该写并没有对应的spanner client端的写).

# 4.1 时间戳管理
表2列出了Spanner支持的操作类型。Spanner实现支持读-写事务，只读事务（预先声明的快照隔离事务），以及快照读。独立写被按照读-写事务的方式实现；非-快照的独立的读以只读事务的方式实现。两者内部都是通过重试实现（client不需要写重试次数）。
 
一个只读事务是 受益于快照隔离技术获得高性能的事务。一个只读事务必须通过预先申明不能有任何写；一个读-写事务没有任何写非常难。只读事务的读在一个系统-选定的时间戳执行并不需要锁，所有到来的写也不会被阻塞。只读事务中的读可以被足够新的副本的任何一个副本处理。
 
一个快照读是无需锁即执行的对过去数据的读。一个client可以或者配置快照读的时间戳，或者提供预期的时间戳无效上界并且让Spanner选择一个时间戳。在任何一种场景下，一个快照读的执行可以在任何足够新的副本执行。
 
 对于这两类只读事务和快照读，一旦时间戳被选中提交是不可避免的。除非在那个时间戳数据已经被回收。所以，客户端可以避免在retry loop中对结果进行缓冲。当一个server异常时，client可以在不通的节点用相同的时间戳以及当前读偏移进行内部重试。
 
 # 4.1.1. Paxos 主选举
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
