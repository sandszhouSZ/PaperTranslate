# Bigtable：一种面向结构化数据的分布式式存储系统

# 前沿
Bigtable是一种设计于处理海量结构化数据的分布式存储系统：这些数据分布于成千上万台通用服务器上。Google的很多项目都是用Bigtable来进行底层数据存储,包括：网页索引，Google地图，Goole金融。这些应用对Bigtable提出了不同的需求：包括数据量（从URLS到网页到卫星图片）和延迟需求（从后端批量处理到实时数据服务）。尽管需求多种多样，但是Bigtable已经为所有的Google的产品成功的提供了灵活的、高性能的解决方案。这篇文章中，我们将描述Bigtable提供的一种简单的数据模型，其为client端提供了关于数据布局和格式的动态灵活的控制，而且我们将对Bigtable的设计和实施细节进行详细描述。

# 1 介绍
过去的两年半时间内，我们成功设计、实施、部署了一种管理结构化数据的分布式存储系统，称之为Bigtable。Bigtable被设计为可以将数据可靠的扩展到PB并且分布在数千台机器上。Bigtable已经实现了下面几个目标：`适用性广泛`，`可扩展性强`，`高性能`,`高可用`。Bigtable至少为Google公司超过6个产品提供存储服务，包括Google分析，Google金融，社交网络服务Orkut，个性化搜索，在线文档编辑Writely,Google地图。这些产品将Bigtable应用于一系列严苛的负载场景，从面向高吞吐的批处理任务到面向终端用户的低延迟服务。这些产品使用的Bigtable集群配置千变万化：从几台到成千台节点，最多存储数百PB的数据。

Bigtable在很多方面都酷似数据库：它拥有和数据库类似的很多实现策略。并行数据库和内存数据库已经拥有很好的可扩展性和高性能，但是相比较而言，Bigtable提供了不同的接口。Bigtable不提供`完整的关系数据模型`；而是基于数据分布和格式之上为client端提供了一种简单的数据模型去动态修改，而且让client 在基于底层存储之上建立数据的局部特性。数据按照行和列名字进行索引，行和列可以被为任意字符串。尽快client会经常将多种结构化数据或者半结构化数据进行串行化为字符串，但是Bigtable将该数据看作为原始数据（并不理解内部细节）。通过仔细的选择模式client可以控制数据布局。最后，Bigtable模式参数允许client对`是从内存还是磁盘提取数据`进行动态的控制。

第2部分对数据模型进行更详细的描述，第3部分我们对client API进行大致的描述。第4部分简单介绍Bigtable依赖的底层的基础服务。第5部分对Bigtable实现的关键特性进行描述。第6部分描述为了提升Bigtable的性能我们所做的改进。第7部分对Bigtable的性能进行评估。第8部分Google内部的几个服务是如何使用使用Bigtable的。第9部分讲述从Bigtable的设计和运营中我们学到的一些经验。最后，在第10部分描述了相关工作。第11部分得出最后的结论。

# 2 数据模型
Bigtable是一个稀疏的、分布式的、持久化的 多维度有序数组。 该数组通过行<关键字，列关键字，时间戳>进行索引；数组中每个数据是一个原生的字节流。即：(row:string, column:string, time:int64) -> string。

我们在对很多类似Bigtable的系统的使用进行大量调研之后确定这种数据模型。作为驱动我们设计策略的一个具体的例子，假设我们想保留一份`海量网页和其相关数据`用作其他不同的项目。让我们将这个特殊的表称之为：Webtable。在Webtable中，我们可以使用URL作为行主键，网页的不同的特性作为列名并且将网页的内容保存在`contents`中：当他们被抓取时选择时间戳之下的列，如表1所示：
![Bigtable存储](https://github.com/sandszhouSZ/PaperTranslate/blob/master/image/Bigtable%E6%A0%BC%E5%BC%8F.png)

## 2.1 行
表中的行关键字是任意字符串（当前最大64KB，大多数场景下一般在10-100字节之间）。每个行的读或写都是原子的（不考虑同一行同时读或者写的不同列的数量），这种设计使得client端对单行在并发更新时对系统的表现很容易达成一致。

Bigtable按照行关键字的字典序维护数据。一张完整的表是按照行的区间被动态的划分为多个分片，每个分片称为：tablet，这是数据分布和均衡的基本单位。这样，对于少量连续行的读只需要和少量的节点交互因此非常高效。client可以通过精心选择行关键字来利用这个特性从而使得数据访问具有很好的局部性。比如，在Webtable中，同一域名下的网页通过反转URL的主机部分可以在系统中进行连续的存储。比如，我们将maps.google.com/index.html 域名的数据的key设置为：com.google.maps/index 。这样同样域名的网页存储在临近的位置使得按照主机以及域名的分析变得非常高效。

## 2.2 列族
列关键字被按组分为一系列集合：列族，列族进行基本的访问控制。按照列族进行排序后，同一列族的数据拥有相同的类型（我们按列族单位进行压缩）。一个列族必须首先创建，之后数据才能在那一列族写入数据；当一个列族创建完毕，其内的所有的列关键字都可以被使用。我们的目标是表中不同的列族数量尽量小(最多几百个)，并且列族在后续操作中基本不变。作为对比，一个表可以有无限数量的列。

一个列关键字使用下面的语法进行命名： family:qualifier。列族的名字必须是可打印的，但是限定符可以为任意字符串。Webtable的一个样例列族为：language，描述了该网页是用哪个语义编写的。我们在language列族中仅仅使用一个列关键字，并且存储了每个网页的语言ID编号。另外一个有用的列族是anchor；该列族下的每个列关键字代表了一个单独的anchor，如表1所示，限定符是引用的网址的名字；cell的内容是连接的文本。

`访问控制，磁盘和内存统计都是按照列族为单位实施的`。在我们的Webtable例子中，这些控制允许我们管理几种不同类型的应用：比如添加新的基础数据，读基础的数据并且创建扩展的列族，一些仅仅允许读取已经存在的数据（甚至由于隐私原因对所有现存的列族都没有只读权限）

## 2.3 时间戳

Bigtable的每个单元都可能包含一份数据的多个版本；这些版本通过时间戳来索引。Bigtable 时间戳是一个64位整数。可以通过Bigtable来进行分配，这种情况下其代表微秒级别的“实时时间”，当然也可以直接通过client端来指定。应用程序必须自身保证生成唯一的时间戳避免发生冲突。一个cell数据的不同版本按照降序进行存储，所以最新的数据会被最先读到。

为了让多版本数据管理尽量简单，Bigtable支持自动回收cell的过期版本，目前Bigtable支持per-column-family粒度的两种配置方式：client可以通过配置保留cell的最近N个版本或者足够新的版本(只保留最近几天写的数据)。

在我们的Webtable样例中，我们将爬取的数据的时间戳设置在Contents列族：对应这些版本的数据真实爬取的时间。上文描述的垃圾回收机制让我们对每个网页只保留最新的若干个版本。

# 3 API
Bigtable的API提供了创建、删除 表和列族的函数。 并且提供了替换 集群，表，列族 元数据的函数，比如访问权限等等。

Client应用程序可以写或者删除Bigtable中的数据，从特定的行查找数据，或者遍历表中的一部分数据。表2展示了 使用RowMutation抽象接口提供一系列更新的C++代码。（为了使样子足够简单不相关的细节被忽略）。Apply接口的调用在Webtable上执行一个原子修改：向www.cnn.com 添加了一个anchor并且删除了不同的anchor。
![Bigtable操作API](https://github.com/sandszhouSZ/PaperTranslate/blob/master/image/Bigtable%E6%93%8D%E4%BD%9C.png)

表3展示了利用Scanner抽象接口去遍历特定行的所有anchors的c++代码片段，这里有几种限制行、列，时间戳的机制。比如，我们可以通过约束只返回匹配正则表达式anchor:*.cnn.com的anchors 或者 只返回timestamp落在十天内的anchors。

Bigtable支持其他的一些特性以便用户可以以非常复杂的方式操作数据。`首先，虽然Bigtable提供多行间批量写的接口`，`Bigtable也支持单行的原子事务，这可以被用来允许Client执行单行key下的原子的读-修改-写操作序列`。另外，`Bigtable允许cell被按照整数计数器去使用`。最后`，Bigtable支持client端的脚本在节点地址空间中执行`，这些脚本是用 Google专门处理数据的语言Sawzall进行编码。当前，我们基于Sawzall的API不允许客户端回写Bigtable，但是允许很多形式的数据转换，基于任意表达式的过滤，经过很多操作级联的概括。

Bigtable可以和MapReduce一起使用来进行Google的大规模并行计算。我们已经编写了很多封装来让Bigtable可以被当作输入源同时作为MapReduce任务的输出结果。

# 4 SSTable结构
Bigtable构建在其他的一些Google基础设施之上。Bigtable使用分布式文件系统GFS来存储数据文件。一个Bigtable集群一般运行于一组共享的机器池中为大量的不同业务场景的应用提供服务。并且Bigtable进程和其他应用进程共享机器。Bigtable依赖一个集群管理系统进行任务调度，资源管理，状态监控。

Google SSTable文件格式被用于存储Bigtable数据。一个SSTable提供一个持久的，有序的不可更改的从key到value的映射，其中key和value都是任意的字节序列。Bigtable提供查找特定key的数据、遍历一段key范围所有的key-value对的接口。在内部，每个SSTable包含一系列块（一般每个块为64KB，可配）。一个块的索引（在SSTable的最后进行存储）被用来对block进行定位；当SSTable打开时，索引被加载进内存。一次查找仅仅需要一次磁盘查询：首先我们通过对内存索引的二分查找确定合理的块，其次从磁盘读取合适的块。一个SSTable也可以全部加载到内存中，这样遍历和查找就可以不用访问内存。

Bigtable依赖高可用和持久性的分布式锁服务Chubby。一个 Chubby服务包含5个有效的节点。一个会被选举为master并响应外部请求。当大多数节点存活并且能相互交互时服务就可用。Chunbby使用Paxos算法来保证节点异常时所有备份数据的一致性。Chubby提供包括目录和小文件的命名空间。每个目录或者文件可以被当作锁，对齐进行读和写都是原子。Chubby client库对Chubby文件提供一致性缓存。每个chubby client于chubby 服务之间都维护了一个session。一个client的会话在租约过期时间间隔内如果未能及时更新session租约时 会过期。当一个client的会话过期，该client将失去所有的锁和打开的句柄。chubby也能对chubby文件或者目录注册回调函数来及时更新自己维护的状态或者会话过期。

Bigtable使用Chubby来完成很多任务： 【锁能力】保证系统任何时刻最多只有一个有效的Master；【存储能力】保存Bigtable引导位置的数据（见5.1）；【锁能力】发现tablet servers上线以及确定tablet server的死亡（5.2）；【存储能力】存储Bigtable的概要数据（每个表列族信息）；【存储能力】存储访问控制列表。如果Chubby一段时间变得不可用，Bigtable也会服务对外提供服务。我们最近在跨越11个chubby服务实例的14个Bigtable集群进行评测。由于Chubby不可用导致的Bigtable中的一些数据不可用的平均时常比例为0.0047%（这些不可用或者是由于Chubby不可用或者网络问题）。单个集群被影响的时常占比大致为0.0326%。

# 5 实现
Bigtable的实现由三个主要部分组长：嵌入client端的库，一个Master服务，部署于多节点的Tablet服务。Tablet服务可以动态的加入（移除）集群来适应工作负载的变化。

master负责:1. tablet到tablet 服务的分配;2. 检测tablet服务的添加和过期;3. 平衡各个tablet服务的负载;4. 回收GFS文件;5. 同时也负责处理类似表或者列族创建这类schema变化。

每个tablet 服务管理了一系列分片（每个tablet server管理数十个到上千个分片）。tablet服务处理其加载的tablet上的读和写请求并在分片太大时进行分类。

像很多其他单Master的分布式存储系统，client不能将master作为数据流的关键路径：clients 直接和分片服务交互进行读和写操作。因为Bigtable并不依靠master节点去定位分片的位置，大多数client永远不用和master服务交互。所以，master实际上负载非常轻。

一个Bigtable集群存储了很多表。每个表包括一系列分片，每个分片包含了一段连续范围行上的所有数据。开始，每个表仅仅包含一个分片。当表逐步增大，会自动的分裂为多个分片，每个分片一般在100-200MB之间。

## 5.1 分片定位
我们使用一个三层的类B+树结构去存储分片定位的信息（表4）。
![定位](https://github.com/sandszhouSZ/PaperTranslate/blob/EditBranch/image/Bigtable%E5%AE%9A%E4%BD%8D.png)
```
图示解读：
Bigtable分片的存储是是全局表映射管理。

对于索引来讲，一个分片就是一个表，所以这里根表 = 根分片。

chubby文件中保存了Bigtable根表的位置，该根表（METADATA表/根分片/128MB）存储于GFS中，索引了GFS系统所有分片的位置信息；
加载该根表后，大概包含了128K个二级表（二级分片 / METADATA表），其都存储于GFS系统中；
加载一个二级分片表后，其内部又包含了128KB个用户分片（每个表分裂的的分片）；

```
第一层是根分片的位置，其作为一个文件存储在Chubby中。根分片在一个特殊的METADATA表中存储了所有分片的位置信息。每个METADATA分片包含了一系列用户分片。
根分片是METADATA表中的第一个分片，其特殊性在于永远不会分裂，这样可以保证分片定位的层次不会超过3层。

METADATA表 用以存储分片的位置，其`行key是通过该分片表的标识符和结束行编码而来`。每个METADATA行在内存占用大致1KB的数据量。假定将METADATA表的内存限制在128MB，我们的三层定位架构可以有效的支持2的34次方（16GB）个分片（在分片大小为128MB时，存储容量为2的61次方的字节量 = 2EB）。

The client library caches
