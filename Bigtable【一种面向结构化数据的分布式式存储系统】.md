# Bigtable：一种面向结构化数据的分布式式存储系统

# 前沿
Bigtable是一种设计于处理海量结构化数据的分布式存储系统：这些数据分布于成千上万台通用服务器上。Google的很多项目都是用Bigtable来进行底层数据存储,包括：网页索引，Google地图，Goole金融。这些应用对Bigtable提出了不同的需求：包括数据量（从URLS到网页到卫星图片）和延迟需求（从后端批量处理到实时数据服务）。尽管需求多种多样，但是Bigtable已经为所有的Google的产品成功的提供了灵活的、高性能的解决方案。这篇文章中，我们将描述Bigtable提供的一种简单的数据模型，其为client端提供了关于数据布局和格式的动态灵活的控制，而且我们将对Bigtable的设计和实施细节进行详细描述。

# 1 介绍
过去的两年半时间内，我们成功设计、实施、部署了一种管理结构化数据的分布式存储系统，称之为Bigtable。Bigtable被设计为可以将数据可靠的扩展到PB并且分布在数千台机器上。Bigtable已经实现了下面几个目标：`适用性广泛`，`可扩展性强`，`高性能`,`高可用`。Bigtable至少为Google公司超过6个产品提供存储服务，包括Google分析，Google金融，社交网络服务Orkut，个性化搜索，在线文档编辑Writely,Google地图。这些产品将Bigtable应用于一系列严苛的负载场景，从面向高吞吐的批处理任务到面向终端用户的低延迟服务。这些产品使用的Bigtable集群配置千变万化：从几台到成千台节点，最多存储数百PB的数据。

Bigtable在很多方面都酷似数据库：它拥有和数据库类似的很多实现策略。并行数据库和内存数据库已经拥有很好的可扩展性和高性能，但是相比较而言，Bigtable提供了不同的接口。Bigtable不提供`完整的关系数据模型`；而是基于数据分布和格式之上为client端提供了一种简单的数据模型去动态修改，而且让client 在基于底层存储之上建立数据的局部特性。数据按照行和列名字进行索引，行和列可以被为任意字符串。尽快client会经常将多种结构化数据或者半结构化数据进行串行化为字符串，但是Bigtable将该数据看作为原始数据（并不理解内部细节）。通过仔细的选择模式client可以控制数据布局。最后，Bigtable模式参数允许client对`是从内存还是磁盘提取数据`进行动态的控制。

第2部分对数据模型进行更详细的描述，第3部分我们对client API进行大致的描述。第4部分简单介绍Bigtable依赖的底层的基础服务。第5部分对Bigtable实现的关键特性进行描述。第6部分描述为了提升Bigtable的性能我们所做的改进。第7部分对Bigtable的性能进行评估。第8部分Google内部的几个服务是如何使用使用Bigtable的。第9部分讲述从Bigtable的设计和运营中我们学到的一些经验。最后，在第10部分描述了相关工作。第11部分得出最后的结论。

# 2 数据模型
Bigtable是一个稀疏的、分布式的、持久化的 多维度有序数组。 该数组通过行<关键字，列关键字，时间戳>进行索引；数组中每个数据是一个原生的字节流。即：(row:string, column:string, time:int64) -> string。

我们在对很多类似Bigtable的系统的使用进行大量调研之后确定这种数据模型。作为驱动我们设计策略的一个具体的例子，假设我们想保留一份`海量网页和其相关数据`用作其他不同的项目。让我们将这个特殊的表称之为：Webtable。在Webtable中，我们可以使用URL作为行主键，网页的不同的特性作为列名并且将网页的内容保存在`contents`中：当他们被抓取时选择时间戳之下的列，如表1所示：
![Bigtable存储](https://github.com/sandszhouSZ/PaperTranslate/blob/master/image/Bigtable%E6%A0%BC%E5%BC%8F.png)

## 2.1 行
表中的行关键字是任意字符串（当前最大64KB，大多数场景下一般在10-100字节之间）。每个行的读或写都是原子的（不考虑同一行同时读或者写的不同列的数量），这种设计使得client端对单行在并发更新时对系统的表现很容易达成一致。

Bigtable按照行关键字的字典序维护数据。一张完整的表是按照行的区间被动态的划分为多个分片，每个分片称为：tablet，这是数据分布和均衡的基本单位。这样，对于少量连续行的读只需要和少量的节点交互因此非常高效。client可以通过精心选择行关键字来利用这个特性从而使得数据访问具有很好的局部性。比如，在Webtable中，同一域名下的网页通过反转URL的主机部分可以在系统中进行连续的存储。比如，我们将maps.google.com/index.html 域名的数据的key设置为：com.google.maps/index 。这样同样域名的网页存储在临近的位置使得按照主机以及域名的分析变得非常高效。

## 2.2 列族
列关键字被按组分为一系列集合：列族，列族进行基本的访问控制。按照列族进行排序后，同一列族的数据拥有相同的类型（我们按列族单位进行压缩）。一个列族必须首先创建，之后数据才能在那一列族写入数据；当一个列族创建完毕，其内的所有的列关键字都可以被使用。我们的目标是表中不同的列族数量尽量小(最多几百个)，并且列族在后续操作中基本不变。作为对比，一个表可以有无限数量的列。

一个列关键字使用下面的语法进行命名： family:qualifier。列族的名字必须是可打印的，但是限定符可以为任意字符串。Webtable的一个样例列族为：language，描述了该网页是用哪个语义编写的。我们在language列族中仅仅使用一个列关键字，并且存储了每个网页的语言ID编号。另外一个有用的列族是anchor；该列族下的每个列关键字代表了一个单独的anchor，如表1所示，限定符是引用的网址的名字；cell的内容是连接的文本。

`访问控制，磁盘和内存统计都是按照列族为单位实施的`。在我们的Webtable例子中，这些控制允许我们管理几种不同类型的应用：比如添加新的基础数据，读基础的数据并且创建扩展的列族，一些仅仅允许读取已经存在的数据（甚至由于隐私原因对所有现存的列族都没有只读权限）

## 2.3 时间戳

Bigtable的每个单元都可能包含一份数据的多个版本；这些版本通过时间戳来索引。Bigtable 时间戳是一个64位整数。可以通过Bigtable来进行分配，这种情况下其代表微秒级别的“实时时间”，当然也可以直接通过client端来指定。应用程序必须自身保证生成唯一的时间戳避免发生冲突。一个cell数据的不同版本按照降序进行存储，所以最新的数据会被最先读到。

为了让多版本数据管理尽量简单，Bigtable支持自动回收cell的过期版本，目前Bigtable支持per-column-family粒度的两种配置方式：client可以通过配置保留cell的最近N个版本或者足够新的版本(只保留最近几天写的数据)。

在我们的Webtable样例中，我们将爬取的数据的时间戳设置在Contents列族：对应这些版本的数据真实爬取的时间。上文描述的垃圾回收机制让我们对每个网页只保留最新的若干个版本。

# 3 API
Bigtable的API提供了创建、删除 表和列族的函数。 并且提供了替换 集群，表，列族 元数据的函数，比如访问权限等等。

Client应用程序可以写或者删除Bigtable中的数据，从特定的行查找数据，或者遍历表中的一部分数据。表2展示了 使用RowMutation抽象接口提供一系列更新的C++代码。（为了使样子足够简单不相关的细节被忽略）。Apply接口的调用在Webtable上执行一个原子修改：向www.cnn.com 添加了一个anchor并且删除了不同的anchor。
![Bigtable操作API](https://github.com/sandszhouSZ/PaperTranslate/blob/master/image/Bigtable%E6%93%8D%E4%BD%9C.png)

表3展示了利用Scanner抽象接口去遍历特定行的所有anchors的c++代码片段，这里有几种限制行、列，时间戳的机制。比如，我们可以通过约束只返回匹配正则表达式anchor:*.cnn.com的anchors 或者 只返回timestamp落在十天内的anchors。

Bigtable支持其他的一些特性以便用户可以以非常复杂的方式操作数据。`首先，虽然Bigtable提供多行间批量写的接口`，`Bigtable也支持单行的原子事务，这可以被用来允许Client执行单行key下的原子的读-修改-写操作序列`。另外，`Bigtable允许cell被按照整数计数器去使用`。最后`，Bigtable支持client端的脚本在节点地址空间中执行`，这些脚本是用 Google专门处理数据的语言Sawzall进行编码。当前，我们基于Sawzall的API不允许客户端回写Bigtable，但是允许很多形式的数据转换，基于任意表达式的过滤，经过很多操作级联的概括。

Bigtable可以和MapReduce一起使用来进行Google的大规模并行计算。我们已经编写了很多封装来让Bigtable可以被当作输入源同时作为MapReduce任务的输出结果。

# 4 SSTable结构
Bigtable构建在其他的一些Google基础设施之上。Bigtable使用分布式文件系统GFS来存储数据文件。一个Bigtable集群一般运行于一组共享的机器池中为大量的不同业务场景的应用提供服务。并且Bigtable进程和其他应用进程共享机器。Bigtable依赖一个集群管理系统进行任务调度，资源管理，状态监控。

Google SSTable文件格式被用于存储Bigtable数据。一个SSTable提供一个持久的，有序的不可更改的从key到value的映射，其中key和value都是任意的字节序列。Bigtable提供查找特定key的数据、遍历一段key范围所有的key-value对的接口。在内部，每个SSTable包含一系列块（一般每个块为64KB，可配）。一个块的索引（在SSTable的最后进行存储）被用来对block进行定位；当SSTable打开时，索引被加载进内存。一次查找仅仅需要一次磁盘查询：首先我们通过对内存索引的二分查找确定合理的块，其次从磁盘读取合适的块。一个SSTable也可以全部加载到内存中，这样遍历和查找就可以不用访问内存。

Bigtable依赖高可用和持久性的分布式锁服务Chubby。一个 Chubby服务包含5个有效的节点。一个会被选举为master并响应外部请求。当大多数节点存活并且能相互交互时服务就可用。Chunbby使用Paxos算法来保证节点异常时所有备份数据的一致性。Chubby提供包括目录和小文件的命名空间。每个目录或者文件可以被当作锁，对齐进行读和写都是原子。Chubby client库对Chubby文件提供一致性缓存。每个chubby client于chubby 服务之间都维护了一个session。一个client的会话在租约过期时间间隔内如果未能及时更新session租约时 会过期。当一个client的会话过期，该client将失去所有的锁和打开的句柄。chubby也能对chubby文件或者目录注册回调函数来及时更新自己维护的状态或者会话过期。

Bigtable使用Chubby来完成很多任务： 【锁能力】保证系统任何时刻最多只有一个有效的Master；【存储能力】保存Bigtable引导位置的数据（见5.1）；【锁能力】发现tablet servers上线以及确定tablet server的死亡（5.2）；【存储能力】存储Bigtable的概要数据（每个表列族信息）；【存储能力】存储访问控制列表。如果Chubby一段时间变得不可用，Bigtable对外提供的服务也会受到影响。我们最近在跨越11个chubby服务实例的14个Bigtable集群进行评测。由于Chubby不可用导致的Bigtable中的一些数据不可用的平均时常比例为0.0047%（这些不可用或者是由于Chubby不可用或者网络问题）。单个集群被影响的时常占比大致为0.0326%。

# 5 实现
Bigtable的实现由三个主要部分组长：嵌入client端的库，一个Master服务，部署于多节点的Tablet服务。Tablet服务可以动态的加入（移除）集群来适应工作负载的变化。

master负责:1. tablet到tablet 服务的分配;2. 检测tablet服务的添加和过期;3. 平衡各个tablet服务的负载;4. 回收GFS文件;5. 同时也负责处理类似表或者列族创建这类schema变化。

每个tablet 服务管理了一系列分片（每个tablet server管理数十个到上千个分片）。tablet服务处理其加载的tablet上的读和写请求并在分片太大时进行分类。

像很多其他单Master的分布式存储系统，client不能将master作为数据流的关键路径：clients 直接和分片服务交互进行读和写操作。因为Bigtable并不依靠master节点去定位分片的位置，大多数client永远不用和master服务交互。所以，master实际上负载非常轻。

一个Bigtable集群存储了很多表。每个表包括一系列分片，每个分片包含了一段连续范围行上的所有数据。开始，每个表仅仅包含一个分片。当表逐步增大，会自动的分裂为多个分片，每个分片一般在100-200MB之间。

## 5.1 分片定位
我们使用一个三层的类B+树结构去存储分片定位的信息（表4）。
![定位](https://github.com/sandszhouSZ/PaperTranslate/blob/EditBranch/image/Bigtable%E5%AE%9A%E4%BD%8D.png)
```
图示解读：
Bigtable分片的存储是是全局表映射管理。

对于索引来讲，一个分片就是一个表，所以这里根表 = 根分片。

chubby文件中保存了Bigtable根表的位置，该根表（METADATA表/根分片/128MB）存储于GFS中，索引了GFS系统所有分片的位置信息；
加载该根表后，大概包含了128K个二级表（二级分片 / METADATA表），其都存储于GFS系统中；
加载一个二级分片表后，其内部又包含了128KB个用户分片（每个表分裂的的分片）；

```
第一层是根分片的位置，其作为一个文件存储在Chubby中。根分片在一个特殊的METADATA表中存储了所有分片的位置信息。每个METADATA分片包含了一系列用户分片。
根分片是METADATA表中的第一个分片，其特殊性在于永远不会分裂，这样可以保证分片定位的层次不会超过3层。

METADATA表 用以存储分片的位置，其`行key是通过该分片表的标识符和结束行编码而来`。每个METADATA行在内存占用大致1KB的数据量。假定将METADATA表的内存限制在128MB，我们的三层定位架构可以有效的支持2的34次方（16GB）个分片（在分片大小为128MB时，存储容量为2的61次方的字节量 = 2EB）。

client端缓存了分片的位置。如果一个client不知道分片的位置或者发现缓存的分片位置信息不正确，其会递归的逐层同步分片位置信息。如果一个client的缓存为空，位置查找算法需要3个网络交互，包括从chubby的一次读。如果client的cache失效，位置查找算法可能需要6次网络交互，因为过期的缓存项只有在不命中时才会发现（假定METADATA分片不会平凡的移动）。由于分片位置在内存中进行存储所以不用访问再GFS，我们另外通过client库的预取算法在一般情况下可以节省更多时间：当读取METADATA表时，读取更多的分片。

我们也在METADATA表中存储其他附属信息，包括每个分片相关的所有事件的日志（比如当一个服务节点开始使用该分片）。这种信息对调试和性能分析非常会非常有帮助。

## 5.2 分片分配
Master行为：`需要及时探测到server不再对分片提供服务的异常并且及时将这些分片分配给其他节点`。为了探测到节点异常，master周期性的查询每个tablet server的锁状态。如果一个tablet server向master上报锁丢失或者master在经过几次重试后仍然无法和tablet server完成交互，master会尝试获取该tablet server的文件的排他锁。如果master成功获取到锁则表明：chubby服务是存活的并且tablet server或者死亡或者无法连接到chubby，所以master通过删除该tablet server的chubby 文件来确保该tablet server无法继续再提供服务。一旦一个tablet server的锁文件被删除，master就可以将之前分片给该tablet server的所有分片分配到`未分配集合`中。为了保证master和chubby服务之间不会有网络问题，如果master的chubby会话过期会杀死自己。但是如上描述，master的失败不会影响到tablet分片到tablet server的分片。

当一个master被集群管理系统启动，master需要发现当前的tablet分片的分布情况。master启动时会执行下面的步骤：（1） master从chubby服务上获取一个独特的master 锁来防止多个master同时启动。 （2） master扫描chubby上的tablet server目录拉获取当前存储的服务节点列表。 （3） master和每一个tablet server进行通信来确定每个tablet server管理的tablet分片集合。 （4）master 扫描METADATA表发现了解总的分片集合。 当扫描到某一个没有被分配出去的分片时，master将该分片加入到未分配分片集合中，这样这个分片可以被用户后续的节点间分配。

一个复杂性在于只有METADATA分片已经被分片后，METADATA表才能被扫描。因此，如果master在第3部没有发现根分片已经被分配给某一个tablet server，在第4部之前master会将根分片分配给`未分配分片集合`。这样确保根分片会被分配给某个tablet server。因为根分片包含了所有METADATA分片的名字，master在扫描根分片后就知道了所有的METADATA分片。

只有当: (1) 一个表在创建或者删除时、（2） 两个已经存在的分片合并为一个更大的分片、（3） 一个已经存在的分片分裂为两个较小的分片 时存活的分片集合才会发生变化。因为master会初始化所有的分片所以其可以对这些变化进行跟踪。一个tablet server通过在METADATA表中添加一个新的分片记录信息来提交分片分裂行为。当分裂已经提交成功时会触发master。 假如该通知发生丢失（由于tablet server或者由于master死亡，此时master不知道该分裂信息），master在命令tablet serve去加载其刚分裂的分片时会重新探测到这个新的分片。这是因为：tablet server发现在master命令其加载的METADATA表的tablet项只配置了一部分分片时tablet server会将分裂信息通知master。

## 5.3 分片服务
分片的持久状态存储在GFS中，如下表5所示。更新操作会被提交到一个提交日志用来存储redo记录。对一批更新，最新提交的更新在内存中一个排序的buffer（memtable）中进行存储；旧的更新存储在一系列SSTable中。为了恢复分片，一个分片服务首先从METADATA表中读取该分片tablet的metable元数据。这个metadata包含了`组成该分片tablet的一系列SSTable文件`和`一系列redo点（一套恢复点）`，该redo点是包含tablet数据的一个提交文件位置指针。server服务首先将SSTables的索引加载到内存并通过重新执行截止redo点时已提交的更新来重建出memtable。
![格式](https://github.com/sandszhouSZ/PaperTranslate/blob/EditBranch/image/tablet%E6%A0%BC%E5%BC%8F.png)

【写请求】当tablet server收到一个写请求，首先会对格式进行检查，并且确认client端有执行写的权限。tablet server通过读代表权限的chubby文件（该文件总是缓存在Chubby的client cache中）来获得可写权限的client节点列表通过匹配进行权限检查。一个有效的修改操作会被写到一个提交日志中。组提交可以用来提升大量小修改操作时系统的性能，内容会被插入到内存有序buffer中(memtable中)。

【读请求】当tablet server收到一个读请求，首先会执行相似的格式检查和权限检查。tablet server通过对一系列SSTable和内存有序buffer的读合并来完成应用层一个有效的读操作。由于SSTable和memtable都是字典序存储的数据结构，合并可以非常高效的进行。

在tablet分片进行分裂或者合并的同时新的读和写操作可以正常进行。

## 5.4 压缩
关键词：次要压缩（minor compacton） 和 合并压缩（merge compaction） 和 主要压缩 （major compaction）
随着写操作的不断执行memtable逐步增大，当memtable增大到一个阈值，该memtable会被冻结同时创建出一个新的memtable。冻结的memtable会异步转化为一个SSTable并且会写入到GFS中，同时SSTable的索引写入到METADATA分片中，称之为：minor compaction（次要压缩）。次要压缩有两个目标：(1) 减少tablet server的内存使用；（2）当该节点挂掉恢复时节省其从commit日志读取的数据的量。新的读和写请求在合并的同时不会受到影响。

每个次要压缩会创建一个新的SSTable。如果这种情况持续不加控制，读操作可能需要合并任意数量的SSTable。所以，我们通过在后台周期性的执行合并压缩来限制SSTable的数量。一个合并压缩过程为：读取一些SSTables的内容和memtable，然后写入到一个新的SSTable中。输入SSTable和memtable在合并结束后可以删除。

将所有SSTables转为只有1个SSTable的合并压缩称为：主要压缩。由非主要压缩产生的SSTable文件可能包含很多特殊的删除项，这些删除项使得仍然有效的更旧的SSTable中仍然有效的数据可以延期被清理（说人话就是： 更旧的SSTable包含一项有效的，在后来合并的SSTable中该项被删除了，此时新的SSTable仅仅包含一个删除标志）。相对而言，主要合并会产生一个不包含删除数据的干净的SSTable。Bigtable会在后台周期性的遍历所有的tablets文件并且实施主要压缩。
这些主要压缩使得Bigtable可以及时回收删除数据占用的资源，并且确保被删除的数据可以及时的从系统中消失，这对面向敏感数据的业务非常重要。

# 6 改进
为了实现高性能、高可用、高可靠，前面几个章节的实现还需要一些改进。这部分更加详细的描述这些改进

## 6.1 局部组
关键词：局部组：业务可能同时访问的列族集合。 列族： 拥有相同类型，同样控制权限，统计的列集合。
关键词：目标：将访问频繁的、小的单独存储，加速性能
client可以将多个列族（column families，拥有相同的类型）合并为一个局部组（locality groups）。对于每个tablet分片可以针对每个局部组生成一个单独的SSTable。将 一般不一起被业务同时访问的列族分离为不同的局部组可以达到更好的读性能。比如，Webtable中的页元数据(比如语言和校验和)可以在一个局部组中，同时页的内容可以存储在另外一个局部组中：想要读取元数据的应用没有必要读取网页的所有内容。

另外，一些有用的参数调整可以以局部组的角度进行配置。比如，一个局部组可以被声明存储在内存。SSTable对于内存局部组的加载采样懒加载方式。一档加载结束，属于该局部组的列族可以被直接从内存访问，不用在进行磁盘IO。 这种特性对需要频繁访问的少量数据的特性非常有用；我们内部对METADATA表中的定位列族采用这种方式。

## 6.2 压缩
关键词：根据数据特性，选择合理的压缩算法，可以极大的提升压缩效率
client端可以控制SSTables是否对一个局部组进行压缩以及使用哪种压缩格式。用户自定义的压缩格式将适用于每个SSTable块（其大小通过局部组相关的配置调整参数控制）。尽快我们通过对每个块进行单独压缩损失了一些空间，但是我们从只需读取少量的SSTable而无需解压缩全部tablet文件得到受益。很多client使用双轮自定义压缩方案。第一轮使用Bentley and McIlroy's的方案，对一个很大窗口的长公共字符串进行压缩。第二轮使用了快速压缩算法来对小的16KB数据窗口进行去重。两轮压缩都非常快速-在现代机器上可以达到100-200MB/s的编码速度，以及400-1000MB/s的解码速度。

即使我们在选择压缩算法时强调的是速度而不是空间节省率，这种两轮压缩方案执行的非常棒。比如，在Webtable中，我们使用这种压缩方案存储网页内容。在我们的一项试验中，我们以一种压缩的局部组的方式存储了大量的文件。为了测试，我们对每个文档只保存来了一个版本。这种方案将空间降低到10%。这相比基于HTML网页内容的传统的Gzip（25%-33%）方案优势非常明显，这是和Webtable的行布局相关的：所有从一台主机爬下的网页存储在相邻的地方。这让Bentley-McIlroy算法可以识别出来自同一主机的具有类似模版的大量网页。很多应用，不仅仅是Webtable，通过精巧的选择行名称可以实现相似的内容能够聚集存储，因此可以获取很高的压缩效率。当我们对一个数据存储多个版本是压缩效率可以达到更高。

## 6.2 通过cache提升读性能
关键词：高层缓存 -> 扫描cache -> key-value（重复访问）；低层缓存 -> 块cache -> block粒度（顺序访问）
为了提升读性能，tablet server使用了两层cache机制。`扫描cache`是一个缓存`SSTable接口向tablet server返回key-value对`的高层次cache。`块Cache`是一个缓存`从GFS系统读取的SSTable block`的低层次cache。扫描cache在上层应用经常重复读取相同的数据时非常有用。块cache在上层应用倾向于顺序读取数据时时非常有用（顺序读取，从热点行相同的局部组多列之间的随机读）。

## 6.3 布隆过滤器
布隆过来器：通过局部组 在tablet的创建一个布隆过滤器（内存存储），其对应GFS一个单独的SSTable，通过该文件即可判断key在tablet分片中是否存在。
就像5.3描述的，一个读操作需要读取组成该tablet分片的所有sstable数据。如果这些SSTable没有在内存中，我们可能在很多次磁盘IO后最终无功而返。 我们通过`允许client为SSTable单独创建一个特殊的局部组来实现布隆过滤器`这种方式减少了访问磁盘的次数。一个布隆过滤器允许查询:`是否有一个SSTable包含了特定行,列的数据(不通过磁盘)`。对于特定的应用，tablet server为布隆过滤器花费的小量的内存浪费可以极大的减少读请求的磁盘查找。我们对布隆过滤器的实际应用也证明：绝大多数对于不存在行、列的查找都可以不同再访问磁盘。

## 6.4 提交日志实现
如果我们对每个tablet分片使用一个单独的日志文件来存储提交日志，将会有大量的到GFS存储的文件的并发写（日志存储在GFS中）。依赖于底层GFS服务上的文件系统实现，这些写将最终导致底层GFS存储为了写大量的不同文件而触发的大量的磁盘寻址操作。同时，由于组会变得非常小，每个分片单独一个日志文件也会降低组提交优化的性能。为了解决这些问题，我们对每个tablet server分配一个提交日志采用追加修改的方式进行。同一节点上不同tablet分片的修改混合存储在同一个日志文件中。

正常的操作流中使用一个日志提供明显的性能提升，但是会使得恢复变得复杂。当一个tablet server挂掉，其服务的tablet会被很多其他tablet server接管：典型的每个server会加载原节点的一小部分tablet分片。 为了恢复分片的状态，新的分片server需要将原节点对于该分片的写修改进行重放。但是，很多tablet分片的日志都混合存储在同一个物理文件中。每个打开只会读取和应用特定tablet分片的日志。但是假定100个节点每个都从原节点接管了一个分片，log文件将被读取100词。

我们通过首先对提交日志进行排序：<table name ,row name,log sequeue number>来避免重复读日志。在排序的输出中，对于同一个tablet的修改将会连续并且通过一次寻址即可顺序的读取所有的数据。为了对排序进行并行化，我们将日志文件按照64MB进行分片，并且在不同的tablet server上进行并发排序。该排序过程在某一个tablet server明确将从一些特定的提交日志文件进行修改重做时被触发（应该是tablet server向master 请求对特定commit log进行重做）并通过master进行调度(读取文件，切片（64M），并发排序，合并)，

向GFS写提交日志有时会出现性能波动，造成这种现象的原因有很多：比如相关的GFS服务器crash，或者将数据传输到GFS 3个备份的网络路径出现拥塞或者过载。为了防止GFS写入延迟出现毛刺，每个tablet server有两个写日志线程，每个写自己单独的日志文件；同一时刻，仅仅有一个线程处于主动状态。如果到日志文件的写入性能变得很差，日志文件的写入将切换到另外一个线程并且到提交日志队列的所有待提交更改将被另外一个线程接管。日志项包含顺序编号，这可以让恢复过程能识别出进程切换并消除期间产生的重复项目。

## 6.5 加速tablet恢复
关键词：次要压缩，停止提供服务，次要压缩，卸载
如果master将一个tablet从一个server迁移到另一个server，(1） 源tablet server首先对该tablet做一个次要压缩，这个压缩可以减少tablet server的提交日志中未提交状态的日志数量。(2) 当次要压缩结束后，tablet server将停止对该tablet提供读写服务。(4) 在他最终将该tablet分片卸载之前。 (3) 其会在做一个次要压缩来减少提交日志中未提交的日志数量（这批日志是在第一次压缩过程中新来的写请求，数据量非常少，所以很快）。当第二次次要压缩结束后，tablet可以被另外一个tablet server接管并且不需要从日志文件中进行恢复。

## 6.6 不变形利用
除了SSTable的cache，很多来自Bigtable系统其他部分都有一个简单的共性：我们产生的所有SSTable都是不可修改的。比如在从SSTable读取数据时我们不需要任何到文件系统的同步访问（说人话就是：不用读盘产生IO，缓存也可以用，因为底层都是只读的）。这样，多行的并发控制可以高效的实现。唯一可以被读操作和写操作可以访问到并且可能修改的数据结构是memtable。为了减少读memtable的冲突，我们对memtable中每个行的修改都执行 copy-and-write，允许读和写并发访问数据。

由于SSTable不能修改，物理上永久删除已经被业务删除的数据的问题将转变为过期SSTable的垃圾回收。每个tablet分片的SSTable都在METAData表中有注册。master
采用标记-清理的垃圾回收策略在SSTable集合中清理过期SSTables，其中METADATA表也包含了一系列roots。

最后，SSTable的不可修改性使得我们可以很快的对tablet进行分裂。我们通过让两个子分片共享父分片的SSTable的方式来快速完成分裂，而不用为每个子分片生成一系列新的SSTable。


